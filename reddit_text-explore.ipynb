{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f3ee05-2485-4dfa-b77d-4a545d8c70a7",
   "metadata": {},
   "source": [
    "## Exploring Reddit data with Python\n",
    "\n",
    "This notebooks contains a couple of examples in how to explore text data (like reddit comments) using text mining techniques in Python.\n",
    "\n",
    "The document you are reading right now is a \"Jupyter Notebook\". This document format combines \"text cells\" (like the one you are reading) with \"code cells\" (like the one below this one). \n",
    "\n",
    "You can recognize a code cell by the squared brackets to the right of it (`[ ]`). These cells can be run by putting the cursor in the cell (clicking into it) and then by either clicking the \"play\" button in the top panel or by pressing \"Shift+Enter\". \n",
    "\n",
    "The notebook represents a workflow, so the cells have to be run in order (the squared brackets to the right indicate the order of which the cells have been run).\n",
    "\n",
    "In some cells, you can make a few adjustment to the code to change the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd22f6e-70a6-4bfc-9cda-28a0d3be5b5d",
   "metadata": {},
   "source": [
    "### Loading packages\n",
    "\n",
    "Python uses \"packages\" to store different functions and tools. A first step is always importing the tools needed for the notebook to work. \n",
    "\n",
    "A few options are also set as well as setting where data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db3b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(20,12)})\n",
    "\n",
    "data_path = os.path.join('.', 'data')\n",
    "out_path = os.path.join('.', 'output')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d26aef-be27-4ccc-ae10-c56843b2d4b9",
   "metadata": {},
   "source": [
    "The next step is loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8be35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "path = os.path.join(data_path, 'reddit_tagpro-mylittlepony_01042017-04042017_long_tokenized.csv')\n",
    "\n",
    "posts_df = pd.read_csv(path)\n",
    "posts_df['tokens'] = posts_df['tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bb97f-5141-472a-b999-80792422be29",
   "metadata": {},
   "source": [
    "With the data loaded, we can inspect it. For example by viewing the first five lines of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fbe2e-f65c-42f3-9cd7-a7786e41b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967a70a-94cd-4ec3-ae15-71233b49bd5a",
   "metadata": {},
   "source": [
    "Or by inspecting the \"shape\" (number of rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d17b0-8e33-41e2-b6d1-dfdb3668279b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783b206-19c6-492b-afc6-e7bf38043f2e",
   "metadata": {},
   "source": [
    "The data contains a range of columns (/variables) for the different type of information stored. The column names gives us a hint as to what the dataset contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0f6bc-e32a-45c1-b2e9-6140ec1b896e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae3ffa-d3e0-4cca-a942-c70bf050b38d",
   "metadata": {},
   "source": [
    "### Data handling\n",
    "\n",
    "Working with data always requires various data handling operations. In the code below, we are telling Python how to work with the datetime information in the data as well as adding additional datetime columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883041a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding date variables\n",
    "\n",
    "posts_df['comment_created_date'] = pd.to_datetime(posts_df['comment_created_utc'], unit = \"s\")\n",
    "posts_df['hour'] = posts_df['comment_created_date'].dt.hour\n",
    "posts_df['comment_date'] = posts_df['comment_created_date'].dt.date\n",
    "posts_df['comment_datehour'] = posts_df['comment_date'].astype('str') + \"-\" + posts_df['hour'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df557cd4-10aa-4ef9-83ed-2885047dff7a",
   "metadata": {},
   "source": [
    "With the datetime information, we can filter the data to include specifically the time period of interest (April 1st to April 4th):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e722533-874e-4f93-9b02-4b0612a97166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fiter comments\n",
    "posts_df = posts_df.loc[(posts_df['comment_created_date'] > '2017-04-01') & (posts_df['comment_created_date'] < '2017-04-05'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affdb07e",
   "metadata": {},
   "source": [
    "## Comment activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ca142",
   "metadata": {},
   "source": [
    "### Visualizing comment activity\n",
    "\n",
    "The comment activity can be visualized. The graph below shows the comment count per day per subreddit.\n",
    "\n",
    "In the cell below, you can change two options:\n",
    "- Whether to look at activity subreddit-wise or across both subreddits\n",
    "- What unit of time to look at\n",
    "\n",
    "Change the values and run the cell to change the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8df9f5-018f-4f54-a359-349679f85bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timeunit = \"date\" # possible options: \"date\", \"hour\"\n",
    "subreddits = \"all\" # possible options: \"compare\", \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa82cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post activity\n",
    "\n",
    "if timeunit == 'hour' and subreddits == 'compare':\n",
    "    groups = ['comment_datehour', 'post_subreddit']\n",
    "elif timeunit == 'date' and subreddits == 'compare':\n",
    "    groups = ['comment_date', 'post_subreddit']\n",
    "elif timeunit == 'hour' and subreddits == 'all':\n",
    "    groups = ['comment_datehour']\n",
    "elif timeunit == 'date' and subreddits == 'all':\n",
    "    groups = ['comment_date']\n",
    "\n",
    "df_timecount = posts_df.groupby(groups).size().to_frame(name = 'count')\n",
    "\n",
    "if len(groups) > 1:\n",
    "    sns.lineplot(data = df_timecount, x = groups[0], y = 'count', hue = 'post_subreddit')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title(\"Reddit comment activity\")\n",
    "else:\n",
    "    sns.lineplot(data = df_timecount, x = groups[0], y = 'count')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title(\"Reddit comment activity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6281b-930b-4521-8a5a-0b6abafdb835",
   "metadata": {},
   "source": [
    "## Analyzing comments using language models\n",
    "\n",
    "One of the many things that machine learning technology has brought us is *language model*. These are models (\"machines\") that are trained to \"understand\" written and spoken human language. \n",
    "\n",
    "Autosuggestions from your messenger apps, speech-to-text (Siri, Google, Alexa) and many other applications use language models.\n",
    "\n",
    "Some of these models are freely available, which allows us to use them to analyze text as well.\n",
    "\n",
    "**Using a language model**\n",
    "\n",
    "In the cells below, you can see an example of how a langauge model works. Change the number to change the comment to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b6da8-9997-4af5-8b33-afdb856935a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change comment number here!\n",
    "\n",
    "comment_no = 123 # a number between 0 and 4185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6e75a-217f-4c34-90af-bf0035a43a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comment = posts_df.loc[comment_no, 'comment_body']\n",
    "\n",
    "print(comment)\n",
    "\n",
    "doc = nlp(comment)\n",
    "\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d26b6a-4986-482c-8028-fb54e3b09389",
   "metadata": {},
   "source": [
    "The language model allows us to create sophisticated \"tokenization-functions\". Tokenization is a common text pre-processing step involving splitting texts into individual words (tokens) while making sure that variations of the same word (fx \"letter\", \"letters\") are treated the same.\n",
    "\n",
    "The cell below defines a custom function for how to pre-process the comment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cafd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "stop_words = list(nlp.Defaults.stop_words)\n",
    "                                            \n",
    "def tokenizer_custom(text, stop_words=stop_words, tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "       \n",
    "    text = text.replace('\\n', ' ')\n",
    "    numbers_re = r\".*\\d.*\"\n",
    "    punct_regex = r\"[^\\w\\s]\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "        \n",
    "    pos_tags = tags # Keeps proper nouns, adjectives and nouns\n",
    "    \n",
    "    exceptions = []\n",
    "    \n",
    "    tokens = []\n",
    "      \n",
    "    for word in doc:\n",
    "        if ((word.pos_ in pos_tags) or (any([exception in word.text for exception in exceptions]))) and (len(word.lemma_) > 2) and (word.lemma_.lower() not in stop_words) and not (re.match(numbers_re, word.lemma_.lower())):\n",
    "            token = word.lemma_.lower() # Returning the word in lower-case.\n",
    "            token = re.sub(punct_regex, \"\", token)\n",
    "            tokens.append(token)\n",
    "\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7963532-69a9-4f66-a36d-9b433f25431d",
   "metadata": {},
   "source": [
    "The cell below applies the function to the data (this takes a while, so the line is \"commented out\". The data read in already includes the tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31ed0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize all data\n",
    "tags = ['NOUN', 'ADJ']\n",
    "\n",
    "#posts_df['tokens'] = posts_df['comment_body'].apply(tokenizer_custom, tags = tags)\n",
    "\n",
    "posts_df_long = posts_df.explode('tokens')\n",
    "posts_df_long['tokens'] = posts_df_long['tokens'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a52d2",
   "metadata": {},
   "source": [
    "### Visualizing the use of words\n",
    "\n",
    "The graph below shows the use of the top X most used words (nouns and adjectives) overall and how it has developed over time (counts per hour).\n",
    "\n",
    "Change the number below to change how many of the top words to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c129b-3ff0-4373-8845-e4e1dd157a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_include = 10 # Change to update the number of words included in visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c80847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing use of top tokens over time\n",
    "\n",
    "top_tokens = list(posts_df_long['tokens'].value_counts().index[0:words_include]) # Top tokens as list\n",
    "df_filter = posts_df_long.loc[posts_df_long['tokens'].isin(top_tokens), :] # Data filtered for top tokens\n",
    "\n",
    "df_timecount = df_filter.groupby(['comment_datehour', 'tokens']).size().to_frame(name = 'count')\n",
    "\n",
    "sns.lineplot(data = df_timecount, x = 'comment_datehour', y = 'count', hue = 'tokens')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(f\"Use of words (top {words_include} overall)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcff15",
   "metadata": {},
   "source": [
    "### Visualizing the use of keywords\n",
    "\n",
    "The use of a specific set of words can also be shown.\n",
    "\n",
    "Tokens are stemmed so that words with the same stem like \"fødevare\", \"fødevarechef\", \"fødevareområdet\", \"fødevaresikkerhed\" are all counted as \"fødevare\".\n",
    "\n",
    "Add keywords to the list below to see the evolution n how they are change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf106a9c-caa3-41ab-8728-8b0aca044378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords = ['place',\n",
    "            'pixel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe91fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing use of specific tokens over time\n",
    "\n",
    "posts_filter = posts_df_long.loc[posts_df_long['tokens'].apply(lambda token: any([word in token for word in keywords])), :] # Data filtered for top tokens\n",
    "\n",
    "for keyword in keywords:\n",
    "    posts_filter.loc[posts_filter['tokens'].str.contains(keyword), 'tokens'] = keyword\n",
    "\n",
    "df_timecount = posts_filter.groupby(['comment_datehour', 'tokens']).size().to_frame(name = 'count')\n",
    "\n",
    "sns.lineplot(data = df_timecount, x = 'comment_datehour', y = 'count', hue = 'tokens')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Use of specific keywords\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8, tm",
   "language": "python",
   "name": "tmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
